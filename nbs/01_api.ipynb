{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# api\n",
    "\n",
    "This is the primary interface to running squ wrappers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas, json, logging\n",
    "from nbdev_squ.core import *\n",
    "from diskcache import memoize_stampede\n",
    "from importlib.resources import path\n",
    "from subprocess import run\n",
    "from azure.monitor.query import LogsQueryClient, LogsBatchQuery, LogsQueryStatus\n",
    "from azure.identity import AzureCliCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.basicConfig(level=logging.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Workspaces\n",
    "\n",
    "The `list_workspaces` function retreives a list of workspaces from blob storage and returns it in various formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@memoize_stampede(cache, expire=60 * 60 * 3) # cache for 3 hours\n",
    "def list_workspaces(fmt: str = \"df\", # df, csv, json, list\n",
    "                    agency: str = \"ALL\"): # Agency alias or ALL\n",
    "    path = datalake_path()\n",
    "    df = pandas.read_csv((path / \"notebooks/lists/SentinelWorkspaces.csv\").open())\n",
    "    df = df.join(pandas.read_csv((path / \"notebooks/lists/SecOps Groups.csv\").open()).set_index(\"Alias\"), on=\"SecOps Group\", rsuffix=\"_secops\")\n",
    "    df = df.rename(columns={\"SecOps Group\": \"alias\", \"Domains and IPs\": \"domains\"})\n",
    "    df = df.dropna(subset=[\"customerId\"]).sort_values(by=\"alias\")\n",
    "    if agency != \"ALL\":\n",
    "        df = df[df[\"alias\"] == agency]\n",
    "    if fmt == \"df\":\n",
    "        return df\n",
    "    elif fmt == \"csv\":\n",
    "        return df.to_csv()\n",
    "    elif fmt == \"json\":\n",
    "        return df.fillna(\"\").to_dict(\"records\")\n",
    "    elif fmt == \"list\":\n",
    "        return list(df[\"customerId\"].unique())\n",
    "    else:\n",
    "        raise ValueError(\"Invalid format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_workspaces().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Analytics Query\n",
    "The below function makes it easy to query all workspaces with sentinel installed using log analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@memoize_stampede(cache, expire=60 * 60 * 3) # cache for 3 hours\n",
    "def list_subscriptions():\n",
    "    return pandas.DataFrame(azcli([\"account\", \"list\"]))[\"id\"].unique()\n",
    "\n",
    "@memoize_stampede(cache, expire=60 * 60 * 3) # cache for 3 hours\n",
    "def list_securityinsights():\n",
    "    return pandas.DataFrame(azcli([\n",
    "        \"graph\", \"query\", \"--first\", \"1000\", \"-q\", \n",
    "        \"\"\"\n",
    "        resources\n",
    "        | where type =~ 'microsoft.operationsmanagement/solutions'\n",
    "        | where name startswith 'SecurityInsights'\n",
    "        | project wlid = tolower(tostring(properties.workspaceResourceId))\n",
    "        | join kind=leftouter (\n",
    "            resources | where type =~ 'microsoft.operationalinsights/workspaces' | extend wlid = tolower(id))\n",
    "            on wlid\n",
    "        | extend customerId = properties.customerId\n",
    "        \"\"\"\n",
    "    ])[\"data\"])\n",
    "\n",
    "def chunks(items, size):\n",
    "    # Yield successive `size` chunks from `items`\n",
    "    for i in range(0, len(items), size):\n",
    "        yield items[i:i + size]\n",
    "\n",
    "@memoize_stampede(cache, expire=60 * 5) # cache for 5 mins\n",
    "def loganalytics_query(queries: list[str], timespan=pandas.Timedelta(\"14d\")):\n",
    "    client = LogsQueryClient(AzureCliCredential())\n",
    "    requests, results = [], []\n",
    "    for query in queries:\n",
    "        for workspace_id in list_securityinsights()[\"customerId\"]:\n",
    "            requests.append(LogsBatchQuery(workspace_id=workspace_id, query=query, timespan=timespan))\n",
    "    querytime = pandas.Timestamp(\"now\")\n",
    "    for request_batch in chunks(requests, 100):\n",
    "        results += client.query_batch(request_batch)\n",
    "    dfs = []\n",
    "    for request, result in zip(requests, results):\n",
    "        if result.status == LogsQueryStatus.PARTIAL:\n",
    "            table = result.partial_data[0]\n",
    "            df = pandas.DataFrame(table.rows, columns=table.columns)\n",
    "        elif result.status == LogsQueryStatus.SUCCESS:\n",
    "            table = result.tables[0]\n",
    "            df = pandas.DataFrame(table.rows, columns=table.columns)\n",
    "        else:\n",
    "            df = pandas.DataFrame([result.__dict__])\n",
    "        df[\"TenantId\"] = request.workspace\n",
    "        df[\"_query\"] = query\n",
    "        df[\"_timespan\"] = timespan\n",
    "        df[\"_querytime\"] = querytime\n",
    "        dfs.append(df)\n",
    "    return pandas.concat(dfs)\n",
    "\n",
    "def query_all(query, fmt=\"df\", timespan=pandas.Timedelta(\"14d\")):\n",
    "    try:\n",
    "        # Check query is not a plain string and is iterable\n",
    "        assert not isinstance(query, str)\n",
    "        iter(query)\n",
    "    except (AssertionError, TypeError):\n",
    "        # if it is a plain string or it's not iterable, convert into a list of queries\n",
    "        query = [query]\n",
    "    df = loganalytics_query(query, timespan)\n",
    "    if fmt == \"df\":\n",
    "        return df\n",
    "    elif fmt == \"csv\":\n",
    "        return df.to_csv()\n",
    "    elif fmt == \"json\":\n",
    "        return df.fillna(\"\").to_dict(\"records\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_securityinsights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def atlaskit_transformer(inputtext, inputfmt=\"md\", outputfmt=\"wiki\", runtime=\"node\", transformer=path(\"nbdev_squ\", \"atlaskit-transformer.bundle.js\").absolute()):\n",
    "    return run([runtime, transformer, inputfmt, outputfmt], input=inputtext, text=True, capture_output=True, check=True).stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(atlaskit_transformer(\"\"\"# Heading 1\n",
    "\n",
    "- a bullet\n",
    "- [a link](https://github.com)\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
