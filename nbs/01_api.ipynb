{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# api\n",
    "\n",
    "This is the primary interface to running squ wrappers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas, json, logging, time, requests\n",
    "from nbdev_squ.core import *\n",
    "from diskcache import memoize_stampede\n",
    "from subprocess import run, CalledProcessError\n",
    "from azure.monitor.query import LogsQueryClient, LogsBatchQuery, LogsQueryStatus\n",
    "from azure.identity import AzureCliCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Workspaces\n",
    "\n",
    "The `list_workspaces` function retreives a list of workspaces from blob storage and returns it in various formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@memoize_stampede(cache, expire=60 * 60 * 3) # cache for 3 hours\n",
    "def list_workspaces(fmt: str = \"df\", # df, csv, json, list\n",
    "                    agency: str = \"ALL\"): # Agency alias or ALL\n",
    "    path = datalake_path()\n",
    "    df = pandas.read_csv((path / \"notebooks/lists/SentinelWorkspaces.csv\").open())\n",
    "    df = df.join(pandas.read_csv((path / \"notebooks/lists/SecOps Groups.csv\").open()).set_index(\"Alias\"), on=\"SecOps Group\", rsuffix=\"_secops\")\n",
    "    df = df.rename(columns={\"SecOps Group\": \"alias\", \"Domains and IPs\": \"domains\"})\n",
    "    df = df.dropna(subset=[\"customerId\"]).sort_values(by=\"alias\").convert_dtypes().reset_index()\n",
    "    if agency != \"ALL\":\n",
    "        df = df[df[\"alias\"] == agency]\n",
    "    if fmt == \"df\":\n",
    "        return df\n",
    "    elif fmt == \"csv\":\n",
    "        return df.to_csv()\n",
    "    elif fmt == \"json\":\n",
    "        return df.fillna(\"\").to_dict(\"records\")\n",
    "    elif fmt == \"list\":\n",
    "        return list(df[\"customerId\"].unique())\n",
    "    else:\n",
    "        raise ValueError(\"Invalid format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use it do lookup an agencies alias based on its `customerId` (also known as `TenantId`, Log Analytics `WorkspaceId`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspaces = list_workspaces()\n",
    "workspaces.query(f'customerId == \"{workspaces[\"customerId\"][0]}\"')[\"alias\"].str.cat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Analytics Query\n",
    "The below function makes it easy to query all workspaces with sentinel installed using log analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@memoize_stampede(cache, expire=60 * 60 * 3) # cache for 3 hours\n",
    "def list_subscriptions():\n",
    "    return pandas.DataFrame(azcli([\"account\", \"list\"]))[\"id\"].unique()\n",
    "\n",
    "@memoize_stampede(cache, expire=60 * 60 * 3) # cache for 3 hours\n",
    "def list_securityinsights():\n",
    "    return pandas.DataFrame(azcli([\n",
    "        \"graph\", \"query\", \"--first\", \"1000\", \"-q\", \n",
    "        \"\"\"\n",
    "        resources\n",
    "        | where type =~ 'microsoft.operationsmanagement/solutions'\n",
    "        | where name startswith 'SecurityInsights'\n",
    "        | project wlid = tolower(tostring(properties.workspaceResourceId))\n",
    "        | join kind=leftouter (\n",
    "            resources | where type =~ 'microsoft.operationalinsights/workspaces' | extend wlid = tolower(id))\n",
    "            on wlid\n",
    "        | extend customerId = properties.customerId\n",
    "        \"\"\"\n",
    "    ])[\"data\"])\n",
    "\n",
    "def chunks(items, size):\n",
    "    # Yield successive `size` chunks from `items`\n",
    "    for i in range(0, len(items), size):\n",
    "        yield items[i:i + size]\n",
    "\n",
    "@memoize_stampede(cache, expire=60 * 5) # cache for 5 mins\n",
    "def loganalytics_query(queries: list[str], timespan=pandas.Timedelta(\"14d\"), batch_size=200, batch_delay=35):\n",
    "    \"\"\"\n",
    "    Run queries across all workspaces, in batches of `batch_size` with a minimum delay of `batch_delay` between batches.\n",
    "    Returns a dictionary of queries and results\n",
    "    \"\"\"\n",
    "    client = LogsQueryClient(AzureCliCredential())\n",
    "    workspaces = list_workspaces(fmt=\"df\")\n",
    "    sentinel_workspaces = list_securityinsights()\n",
    "    requests, results = [], []\n",
    "    for query in queries:\n",
    "        for workspace_id in sentinel_workspaces[\"customerId\"]:\n",
    "            requests.append(LogsBatchQuery(workspace_id=workspace_id, query=query, timespan=timespan))\n",
    "    querytime = pandas.Timestamp(\"now\")\n",
    "    logger.info(f\"Executing {len(requests)} queries at {querytime}\")\n",
    "    for request_batch in chunks(requests, batch_size):\n",
    "        batch_start = pandas.Timestamp(\"now\")\n",
    "        results += client.query_batch(request_batch)\n",
    "        duration = pandas.Timestamp(\"now\") - batch_start\n",
    "        logger.info(f\"Completed {len(results)} in {duration}\")\n",
    "        if duration.total_seconds() < batch_delay:\n",
    "            time.sleep(batch_delay - duration.total_seconds())\n",
    "    dfs = {}\n",
    "    for request, result in zip(requests, results):\n",
    "        if result.status == LogsQueryStatus.PARTIAL:\n",
    "            tables = result.partial_data\n",
    "            tables = [pandas.DataFrame(table.rows, columns=table.columns) for table in tables]\n",
    "        elif result.status == LogsQueryStatus.SUCCESS:\n",
    "            tables = result.tables\n",
    "            tables = [pandas.DataFrame(table.rows, columns=table.columns) for table in tables]\n",
    "        else:\n",
    "            tables = [pandas.DataFrame([result.__dict__])]\n",
    "        df = pandas.concat(tables).dropna(axis=1, how='all') # prune empty columns\n",
    "        df[\"TenantId\"] = request.workspace\n",
    "        alias = workspaces.query(f'customerId == \"{request.workspace}\"')[\"alias\"].str.cat()\n",
    "        if alias == '':\n",
    "            alias = sentinel_workspaces.query(f'customerId == \"{request.workspace}\"')[\"name\"].str.cat()\n",
    "        df[\"_alias\"] = alias\n",
    "        query = request.body[\"query\"]\n",
    "        if query in dfs:\n",
    "            dfs[query].append(df)\n",
    "        else:\n",
    "            dfs[query] = [df]\n",
    "    return {query: pandas.concat(results, ignore_index=True).convert_dtypes() for query, results in dfs.items()}\n",
    "\n",
    "def query_all(query, fmt=\"df\", timespan=pandas.Timedelta(\"14d\")):\n",
    "    try:\n",
    "        # Check query is not a plain string and is iterable\n",
    "        assert not isinstance(query, str)\n",
    "        iter(query)\n",
    "    except (AssertionError, TypeError):\n",
    "        # if it is a plain string or it's not iterable, convert into a list of queries\n",
    "        query = [query]\n",
    "    df = pandas.concat(loganalytics_query(query, timespan).values())\n",
    "    if fmt == \"df\":\n",
    "        return df\n",
    "    elif fmt == \"csv\":\n",
    "        return df.to_csv()\n",
    "    elif fmt == \"json\":\n",
    "        return df.fillna(\"\").to_dict(\"records\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_all([\"SecurityAlert\"]).groupby(\"_alias\")[[\"Tactics\", \"TenantId\"]].nunique().sort_values(by=\"Tactics\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def atlaskit_transformer(inputtext, inputfmt=\"md\", outputfmt=\"wiki\", runtime=\"node\"):\n",
    "    import nbdev_squ\n",
    "    transformer = dirs.user_cache_path / f\"atlaskit-transformer.bundle_v{nbdev_squ.__version__}.js\"\n",
    "    if not transformer.exists():\n",
    "        transformer_url = f'{nbdev_squ._modidx.d[\"settings\"][\"git_url\"]}/releases/download/v{nbdev_squ.__version__}/atlaskit-transformer.bundle.js'\n",
    "        transformer.write_bytes(requests.get(transformer_url).content)\n",
    "    cmd = [runtime, str(transformer), inputfmt, outputfmt]\n",
    "    logger.debug(\" \".join(cmd))\n",
    "    try:\n",
    "        return run(cmd, input=inputtext, text=True, capture_output=True, check=True).stdout\n",
    "    except CalledProcessError:\n",
    "        run(cmd, input=inputtext, text=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(atlaskit_transformer(\"\"\"# Heading 1\n",
    "\n",
    "- a bullet\n",
    "[a link](https://github.com]\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
